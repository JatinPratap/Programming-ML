{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e084e610-8128-4769-ab64-6aa194044892",
    "_uuid": "20c011dd401be7b6448c43f965e5d0bf548c53b9",
    "id": "CbB4DzTUR1lU"
   },
   "source": [
    "\n",
    "Text Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. \n",
    "\n",
    "### Generating News headlines \n",
    "\n",
    "In this kernel, I will be using the dataset of [New York Times Comments and Headlines](https://www.kaggle.com/aashita/nyt-comments) to train a text generation language model which can be used to generate News Headlines\n",
    "\n",
    "\n",
    "## 1. Import the libraries\n",
    "\n",
    "As the first step, we need to import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:40:31.426657Z",
     "start_time": "2021-08-22T16:40:31.420674Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "u8Sa63mNR1lV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout , LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "# from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "# set_random_seed(2)\n",
    "seed(5555)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "WjPOpsj_R1lc"
   },
   "source": [
    "## 2. Load the dataset\n",
    "\n",
    "Load the dataset of news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T15:20:56.464984Z",
     "start_time": "2021-08-22T15:20:56.433063Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcWRtbKTSNlQ",
    "outputId": "71e45813-54a9-43fc-dfee-e72ae4a657f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "      <th>articleWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a974697410cf7000162e8a4</td>\n",
       "      <td>By BINYAMIN APPELBAUM</td>\n",
       "      <td>article</td>\n",
       "      <td>Virtual Coins, Real Resources</td>\n",
       "      <td>['Bitcoin (Currency)', 'Electric Light and Pow...</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-01 00:17:22</td>\n",
       "      <td>Economy</td>\n",
       "      <td>America has a productivity problem. One explan...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/02/28/business/ec...</td>\n",
       "      <td>1207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a974be7410cf7000162e8af</td>\n",
       "      <td>By HELENE COOPER and ERIC SCHMITT</td>\n",
       "      <td>article</td>\n",
       "      <td>U.S. Advances Military Plans for North Korea</td>\n",
       "      <td>['United States Defense and Military Forces', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Washington</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-03-01 00:40:01</td>\n",
       "      <td>Asia Pacific</td>\n",
       "      <td>The American military is looking at everything...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/02/28/world/asia/...</td>\n",
       "      <td>1215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a9752a2410cf7000162e8ba</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Mr. Trump and the ‘Very Bad Judge’</td>\n",
       "      <td>['Trump, Donald J', 'Curiel, Gonzalo P', 'Unit...</td>\n",
       "      <td>1</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>26</td>\n",
       "      <td>2018-03-01 01:08:46</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Can you guess which man is the model public se...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2018/02/28/opinion/tru...</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a975310410cf7000162e8bd</td>\n",
       "      <td>By JAVIER C. HERNÁNDEZ</td>\n",
       "      <td>article</td>\n",
       "      <td>To Erase Dissent, China Bans Pooh Bear and ‘N’</td>\n",
       "      <td>['China', 'Xi Jinping', 'Term Limits (Politica...</td>\n",
       "      <td>1</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-01 01:10:35</td>\n",
       "      <td>Asia Pacific</td>\n",
       "      <td>Censors swung into action after Mr. Xi’s bid t...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/02/28/world/asia/...</td>\n",
       "      <td>1315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a975406410cf7000162e8c3</td>\n",
       "      <td>By JESSE DRUCKER, KATE KELLY and BEN PROTESS</td>\n",
       "      <td>article</td>\n",
       "      <td>Loans Flowed to Kushner Cos. After Visits to t...</td>\n",
       "      <td>['Kushner, Jared', 'Kushner Cos', 'United Stat...</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-01 01:14:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Apollo, the private equity firm, and Citigroup...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/02/28/business/ja...</td>\n",
       "      <td>1566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID                                        byline  \\\n",
       "0  5a974697410cf7000162e8a4                         By BINYAMIN APPELBAUM   \n",
       "1  5a974be7410cf7000162e8af             By HELENE COOPER and ERIC SCHMITT   \n",
       "2  5a9752a2410cf7000162e8ba                        By THE EDITORIAL BOARD   \n",
       "3  5a975310410cf7000162e8bd                        By JAVIER C. HERNÁNDEZ   \n",
       "4  5a975406410cf7000162e8c3  By JESSE DRUCKER, KATE KELLY and BEN PROTESS   \n",
       "\n",
       "  documentType                                           headline  \\\n",
       "0      article                      Virtual Coins, Real Resources   \n",
       "1      article       U.S. Advances Military Plans for North Korea   \n",
       "2      article                 Mr. Trump and the ‘Very Bad Judge’   \n",
       "3      article     To Erase Dissent, China Bans Pooh Bear and ‘N’   \n",
       "4      article  Loans Flowed to Kushner Cos. After Visits to t...   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Bitcoin (Currency)', 'Electric Light and Pow...           1    Business   \n",
       "1  ['United States Defense and Military Forces', ...           1  Washington   \n",
       "2  ['Trump, Donald J', 'Curiel, Gonzalo P', 'Unit...           1   Editorial   \n",
       "3  ['China', 'Xi Jinping', 'Term Limits (Politica...           1     Foreign   \n",
       "4  ['Kushner, Jared', 'Kushner Cos', 'United Stat...           1    Business   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          1  2018-03-01 00:17:22       Economy   \n",
       "1         11  2018-03-01 00:40:01  Asia Pacific   \n",
       "2         26  2018-03-01 01:08:46       Unknown   \n",
       "3          1  2018-03-01 01:10:35  Asia Pacific   \n",
       "4          1  2018-03-01 01:14:41       Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  America has a productivity problem. One explan...  The New York Times   \n",
       "1  The American military is looking at everything...  The New York Times   \n",
       "2  Can you guess which man is the model public se...  The New York Times   \n",
       "3  Censors swung into action after Mr. Xi’s bid t...  The New York Times   \n",
       "4  Apollo, the private equity firm, and Citigroup...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \\\n",
       "0           News  https://www.nytimes.com/2018/02/28/business/ec...   \n",
       "1           News  https://www.nytimes.com/2018/02/28/world/asia/...   \n",
       "2      Editorial  https://www.nytimes.com/2018/02/28/opinion/tru...   \n",
       "3           News  https://www.nytimes.com/2018/02/28/world/asia/...   \n",
       "4           News  https://www.nytimes.com/2018/02/28/business/ja...   \n",
       "\n",
       "   articleWordCount  \n",
       "0              1207  \n",
       "1              1215  \n",
       "2              1043  \n",
       "3              1315  \n",
       "4              1566  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = pd.read_csv('ArticlesMarch2018.csv')\n",
    "article_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:13:50.776851Z",
     "start_time": "2021-08-22T16:13:50.771867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcWRtbKTSNlQ",
    "outputId": "71e45813-54a9-43fc-dfee-e72ae4a657f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = list([h for h in article_df['headline'].values if h != \"Unknown\"])\n",
    "\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150",
    "_uuid": "fda5d4868631d3618d4d9a9a863541b2faf121c0",
    "id": "HkF21nnDR1lg"
   },
   "source": [
    "## 3. Dataset preparation\n",
    "\n",
    "### 3.1 Dataset cleaning \n",
    "\n",
    "In dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:13:51.597494Z",
     "start_time": "2021-08-22T16:13:51.584030Z"
    },
    "_cell_guid": "b8bf84ed-da11-4f89-a584-9dceea677420",
    "_uuid": "2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agBhQcIvR1lh",
    "outputId": "966a59e5-43ae-49bc-fff4-d474d4534a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virtual coins real resources',\n",
       " 'us advances military plans for north korea',\n",
       " 'mr trump and the very bad judge',\n",
       " 'to erase dissent china bans pooh bear and n',\n",
       " 'loans flowed to kushner cos after visits to the white house',\n",
       " 'china envoy intends to ease trade tensions',\n",
       " 'president trumps contradictory and sometimes false comments about gun policy to lawmakers',\n",
       " 'classic letter puzzle',\n",
       " 'silicon valley disruption in an australian school',\n",
       " 'the assassination of gianni versace episode 6 a nothing man']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af",
    "_uuid": "6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118",
    "id": "tImxs8ZNR1ll"
   },
   "source": [
    "### 3.2 Generating Sequence of N-gram Tokens\n",
    "\n",
    "Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.  \n",
    "\n",
    "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Python’s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:13:53.564193Z",
     "start_time": "2021-08-22T16:13:53.514327Z"
    },
    "_cell_guid": "896543c9-7944-4748-b8ef-ef8cbc2a84f0",
    "_uuid": "9129a8b773feb72eff91aa0025157a173d10c625",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhjzI4eWR1ll",
    "outputId": "b9c8faf2-15d0-498c-8a5d-e71e63ec1ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1119, 1120],\n",
       " [1119, 1120, 116],\n",
       " [1119, 1120, 116, 1121],\n",
       " [31, 1122],\n",
       " [31, 1122, 589],\n",
       " [31, 1122, 589, 392],\n",
       " [31, 1122, 589, 392, 7],\n",
       " [31, 1122, 589, 392, 7, 61],\n",
       " [31, 1122, 589, 392, 7, 61, 70],\n",
       " [117, 10]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    \n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    \n",
    "    # getting coresponding index of words in the sentence\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        \n",
    "        # making combination of words\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "            \n",
    "    return input_sequences, total_words\n",
    "\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:19:22.749763Z",
     "start_time": "2021-08-22T16:19:22.745781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a22c88f5-f2a3-457c-835b-63341e657e3f",
    "_uuid": "f22aa5e0c04620ca5034ab9389322eee543060c6",
    "id": "rY7xLNsXR1lo"
   },
   "source": [
    "In the above output [1119, 1120], [1119, 1120,116], [1119, 1120, 116, 1121] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n",
    "\n",
    "**Headline:** i stand  with the shedevils  \n",
    "**Ngrams:** | **Sequence of Tokens**\n",
    "\n",
    "<table>\n",
    "<tr><td>Ngram </td><td> Sequence of Tokens</td></tr>\n",
    "<tr> <td>i stand </td><td> [30, 507] </td></tr>\n",
    "<tr> <td>i stand with </td><td> [30, 507, 11] </td></tr>\n",
    "<tr> <td>i stand with the </td><td> [30, 507, 11, 1] </td></tr>\n",
    "<tr> <td>i stand with the shedevils </td><td> [30, 507, 11, 1, 975] </td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 Padding the Sequences and obtain Variables : Predictors and Target\n",
    "\n",
    "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label. For example:\n",
    "\n",
    "\n",
    "Headline:  they are learning data science\n",
    "\n",
    "<table>\n",
    "<tr><td>PREDICTORS </td> <td>           LABEL </td></tr>\n",
    "<tr><td>they                   </td> <td>  are</td></tr>\n",
    "<tr><td>they are               </td> <td>  learning</td></tr>\n",
    "<tr><td>they are learning      </td> <td>  data</td></tr>\n",
    "<tr><td>they are learning data </td> <td>  science</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:13:55.278486Z",
     "start_time": "2021-08-22T16:13:55.228626Z"
    },
    "_cell_guid": "73254551-40bd-45b1-a7a5-88fe4cbe0b20",
    "_uuid": "ca588b414e70e21bebcead960f6632805d37dd8c",
    "id": "dFQ8LfydR1lp"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    \n",
    "    # finding the length of the longest sentence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    \n",
    "    # putting zeros to make length of all sentences same\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "    \n",
    "    # selecting the last column as label , and all before that as predictior\n",
    "    predictors, label = input_sequences[:,:-1] , input_sequences[:,-1]\n",
    "    \n",
    "    # making label as categorical\n",
    "    label = ku.to_categorical(label, num_classes = total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:17:13.270040Z",
     "start_time": "2021-08-22T16:17:13.265104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 1119],\n",
       "       [   0,    0,    0, ...,    0, 1119, 1120],\n",
       "       [   0,    0,    0, ..., 1119, 1120,  116],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  979,  151,  386],\n",
       "       [   0,    0,    0, ...,    0,    0, 3581],\n",
       "       [   0,    0,    0, ...,    0, 3581,    5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:17:15.276033Z",
     "start_time": "2021-08-22T16:17:15.271060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:40:58.681127Z",
     "start_time": "2021-08-22T16:40:58.502734Z"
    },
    "_cell_guid": "60d6721e-e40e-4f2b-8f63-c06459d68f26",
    "_uuid": "76ef6d9352002d333a7c75e8aed7ce996015f527",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkIKnEy1R1lu",
    "outputId": "71fb4916-598f-4932-dd78-c889c3f897ef"
   },
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    \n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length = input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1\n",
    "    # model.add(SimpleRNN(200))\n",
    "    model.add(LSTM(200))\n",
    "    # model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation = 'softmax'))\n",
    "\n",
    "    # compileing model\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1826aa1a-cb77-4379-a69d-e9b180945dce",
    "_uuid": "f0b16b471969dbb831cb0024e303341e11b63de4",
    "id": "UdBpZSRzR1lz"
   },
   "source": [
    "Lets train our model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:40:59.840672Z",
     "start_time": "2021-08-22T16:40:59.836687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8057"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:49:26.119962Z",
     "start_time": "2021-08-22T16:45:33.654604Z"
    },
    "_cell_guid": "07d5cf03-d171-4993-9f8b-18446649ecb0",
    "_uuid": "156f3303b8120cc6932e6db985cbea4a7ceb08bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzAPw05OR1lz",
    "outputId": "dcb56977-2e2e-4a24-e27f-3d4c58f7e3ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 1.3867\n",
      "Epoch 2/50\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 1.3502\n",
      "Epoch 3/50\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 1.3218\n",
      "Epoch 4/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 1.2861\n",
      "Epoch 5/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.2534\n",
      "Epoch 6/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.2238\n",
      "Epoch 7/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.1918\n",
      "Epoch 8/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.1648\n",
      "Epoch 9/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.1370\n",
      "Epoch 10/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 1.1079\n",
      "Epoch 11/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 1.0778\n",
      "Epoch 12/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 1.0525\n",
      "Epoch 13/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 1.0241\n",
      "Epoch 14/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.9960\n",
      "Epoch 15/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.9719\n",
      "Epoch 16/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 0.9425\n",
      "Epoch 17/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 0.9191\n",
      "Epoch 18/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.8920\n",
      "Epoch 19/50\n",
      "252/252 [==============================] - 5s 20ms/step - loss: 0.8693\n",
      "Epoch 20/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.8452\n",
      "Epoch 21/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.8245\n",
      "Epoch 22/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.8015\n",
      "Epoch 23/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.7815\n",
      "Epoch 24/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.7578\n",
      "Epoch 25/50\n",
      "252/252 [==============================] - 5s 20ms/step - loss: 0.7391\n",
      "Epoch 26/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.7177\n",
      "Epoch 27/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.6987\n",
      "Epoch 28/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.6876\n",
      "Epoch 29/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.6690\n",
      "Epoch 30/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.6516\n",
      "Epoch 31/50\n",
      "252/252 [==============================] - 5s 20ms/step - loss: 0.6425\n",
      "Epoch 32/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.6187\n",
      "Epoch 33/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.5997\n",
      "Epoch 34/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.5855\n",
      "Epoch 35/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.5731\n",
      "Epoch 36/50\n",
      "252/252 [==============================] - 5s 21ms/step - loss: 0.5571\n",
      "Epoch 37/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.5445\n",
      "Epoch 38/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.5313\n",
      "Epoch 39/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.5212\n",
      "Epoch 40/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.5129\n",
      "Epoch 41/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.5102\n",
      "Epoch 42/50\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 0.5306\n",
      "Epoch 43/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 0.4967\n",
      "Epoch 44/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 0.4715\n",
      "Epoch 45/50\n",
      "252/252 [==============================] - 4s 17ms/step - loss: 0.4562\n",
      "Epoch 46/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.4484\n",
      "Epoch 47/50\n",
      "252/252 [==============================] - 4s 18ms/step - loss: 0.4404\n",
      "Epoch 48/50\n",
      "252/252 [==============================] - 5s 21ms/step - loss: 0.4337\n",
      "Epoch 49/50\n",
      "252/252 [==============================] - 5s 18ms/step - loss: 0.4257\n",
      "Epoch 50/50\n",
      "252/252 [==============================] - 5s 20ms/step - loss: 0.4178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18c1ca6b2b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs = 50, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61e99cfe-7395-4d61-8d1a-8539103d3db5",
    "_uuid": "448bf43b123060dfe4e27cb9f12889e4fe0ed2a7",
    "id": "GcMfWBOuR1l3"
   },
   "source": [
    "## 5. Generating the text \n",
    "\n",
    "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:49:26.135909Z",
     "start_time": "2021-08-22T16:49:26.122949Z"
    },
    "_uuid": "e71e56543b7065f115a05e3fd062262b3b94ad46",
    "id": "xNSRq6vWR1l3"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    \n",
    "    for i in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list, verbose = 0)[0])\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+ output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea0bddb6-acc6-4592-a2e0-ffc4129a582f",
    "_uuid": "c49bf4ea0e54f3145149e164e243d897f545b84c",
    "id": "c2HjPtdHR1l7"
   },
   "source": [
    "## 6. Some Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T16:49:59.351484Z",
     "start_time": "2021-08-22T16:49:58.186314Z"
    },
    "_cell_guid": "e38dd280-093b-4091-b82b-9aa90045b107",
    "_kg_hide-input": true,
    "_uuid": "a21548224c9e661a29e3d369e348aada0599bdc9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9NIohFfR1l9",
    "outputId": "0d689b31-e934-41b6-a8fe-7387bf20151f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Leaves Trump To His Instinct Of\n",
      "President Trump Pushes Tariffs Latin America Links Up\n",
      "Donald Trump Man At War Street As When\n",
      "India And China On One Week At A Time\n",
      "New York Forgets Its Juvenile Lifers Over Trump\n",
      "Science And Technology Trump Latin And Place Of Brokering\n"
     ]
    }
   ],
   "source": [
    "nx = 6\n",
    "print (generate_text(\"united states\", nx, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\",nx, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", nx, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", nx, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", nx, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", nx, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "RNN_Text_generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
