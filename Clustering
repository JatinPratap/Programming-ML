
Axe Bank Credit Card Customer Segmentation
Background: Axe Bank wants to focus on its credit card customer base in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the Marketing team proposes to run personalised campaigns to target new customers as well as upsell to existing customers. Another insight from the market research was that the customers perceive the support services of the back poorly. Based on this, the Operations team wants to upgrade the service delivery model, to ensure that customers queries are resolved faster. Head of Marketing and Head of Delivery both decide to reach out to the Data Science team for help.

Data Description: Data is of various customers of a bank with their credit limit, the total number of credit cards the customer has, and different channels through which customer has contacted the bank for any queries, different channels include visiting the bank, online and through a call centre.

Key Questions:

How many different segments of customers are there?
How are these segments different from each other?
What are your recommendations to the bank on how to better market to and service these customers?
Importing the Libraries
import warnings
warnings.filterwarnings('ignore')
import numpy as np 
import pandas as pd

from sklearn.preprocessing import StandardScaler

import seaborn as sns 
import matplotlib.pyplot as plt

from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans

from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import dendrogram, linkage,cophenet
#Reading the dataset 
df=pd.read_excel('CreditCardCustomerDataSet.xlsx')
df.shape
df.describe()
There are only 655 unique values in "customer Key", it is likely that there are few values that are missing
#number of credit cards owened credit cards so far
df['Total_Credit_Cards'].value_counts()
df['Total_visits_bank'].value_counts()
#pie chart for the target value

plt.figure(figsize= (12,7))
df_target= df['Total_visits_bank'].value_counts()
plt.pie(df_target, labels= df_target.index, autopct= '%.1f%%', startangle= 90, shadow = True )

plt.title('Total visits bank', fontsize= 16)

plt.show()
df['Total_visits_online'].value_counts()
#pie chart for the target value

plt.figure(figsize= (12,12))
df_target= df['Total_visits_online'].value_counts()
plt.pie(df_target, labels= df_target.index, autopct= '%.1f%%', shadow = True )

plt.title('Total visits online', fontsize= 16)

plt.show()
df['Total_calls_made'].value_counts()
#pie chart for the target value

plt.figure(figsize= (12,12))
df_target= df['Total_calls_made'].value_counts()
plt.pie(df_target, labels= df_target.index, autopct= '%.1f%%', shadow = True )

plt.title('Total calls made', fontsize= 16)

plt.show()
pd.crosstab(df['Total_calls_made'], df['Total_visits_bank'],normalize='columns')
pd.crosstab(df['Total_Credit_Cards'], df['Total_visits_bank'],normalize='columns')
#number of unique customers with the bank 
df['Customer Key'].nunique()
df['Customer Key'].value_counts()
df[df['Customer Key']==50706]
df['Customer Key'].drop_duplicates(keep='last').shape
# Dropping duplicates based on unique customer key
df = df.iloc[df['Customer Key'].drop_duplicates(keep='last').index]
df.shape
The cols : Sl_No and CustomerKey are IDs which can be eliminated as they are unique and will not have any relevant role in forming the clusters so we remove them

cols_to_consider=['Avg_Credit_Limit','Total_Credit_Cards','Total_visits_bank','Total_visits_online','Total_calls_made']
subset=df[cols_to_consider]  #Selecting only the above columns 
subset
EDA
Checking for Missing Values
#NullValues with the help of Heatmap

sns.heatmap(subset.isnull(), cmap="inferno")
plt.figure(figsize=(16,9))
No missing values were found

Checking for the statistically summary
subset.describe()
The min and max value of 'Avg_Credit_Limit' is very larger as compared to the other columns To bring the data to the same scale let's standardize the data.

Feature Correlations
# Use Corr function to create correlation matrix
subset.corr()
Plot Correlation Matrix

## Use Seaborn Heatmap to visualize correlation matrix
sns.heatmap(subset.corr(),annot=True,vmin=-1,vmax=1);
Visualize feature distributions
sns.pairplot(subset,diag_kind='kde');
## distribution among continuous values

for i in subset:
    data=subset.copy()
    data[i].hist()
    plt.xlabel(i)
    plt.ylabel('Count')
    plt.title(i)
    plt.show()
Check Skewness
subset.skew()
Log Transformation [Box-Cox Transformation]
subset_2=subset.copy()
# Use Log transformation to scale features

## Hint : use np.log function 

subset_2['Avg_Credit_Limit'] = np.log(subset_2['Avg_Credit_Limit']) 

#can't take log(0) and so add a small number

subset_2['Total_visits_online'] = np.log(subset_2['Total_visits_online']+0.1)
subset_2.skew()
Visualize the Normalized data
# Produce a scatter matrix for each pair of features in the data
sns.pairplot(subset_2,diag_kind='kde');
sns.heatmap(subset_2.corr(),annot=True);
subset_2
Feature Scaling For Standardization - Standard Scaler ( Z Score )
scaler=StandardScaler()
subset_scaled=scaler.fit_transform(subset_2)   
subset_scaled_df=pd.DataFrame(subset_scaled,columns=subset_2.columns)   #Creating a dataframe of the above results
subset_scaled_df
subset_scaled_df.skew()
Execute K-Means Algorithm
## Iterate the K-Means for different values of clusters. Compute the error term and store in an object

cluster_range = range( 1, 15)
cluster_errors = []

for num_clusters in cluster_range:
    clusters = KMeans( num_clusters, n_init = 100,init='k-means++')
    clusters.fit(subset_scaled_df)
    cluster_errors.append( clusters.inertia_ )    # capture the intertia
# combine the cluster_range and cluster_errors into a dataframe by combining them
clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors} )
clusters_df
Elbow Method
plt.figure(figsize=(12,6))
plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = "o" );
Execute the K-Means again with optimal cluster number
kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2345)
kmeans.fit(subset_scaled_df)
centroids = kmeans.cluster_centers_ 
centroids
centroid_df = pd.DataFrame(centroids, columns = subset_scaled_df.columns )
centroid_df
The above are the centroids for the different clusters

Adding Label to the dataset
dataset=subset_scaled_df.copy()  #creating a copy of the data 
dataset
dataset['KmeansLabel']=kmeans.labels_
dataset
dataset.groupby('KmeansLabel').mean()
Customer Profiling - Visualizing the clusters
sns.pairplot(dataset,diag_kind='kde',hue='KmeansLabel');
subset['KmeansLabel']=kmeans.labels_
subset
subset.groupby('KmeansLabel').mean()
The clusters we are visualizing seems to do a good job but the preferred way will be to reduce the dimensions to 3 or less (if possible ) and then try to plot the clusters
HINT: Try PCA before clustering

Analyse the Clusters
Let us make a visualization to observe the different clusters by making boxplots , for the clusters we expect to observe statistical properties which differentiates clusters with each other

dataset.boxplot(by = 'KmeansLabel',  layout=(2,4), figsize=(20, 15))
plt.show()
Looking the box plot we can observe differentiated clusters
#dont have to learn the code, its from the Skikit 

from __future__ import print_function
%matplotlib inline

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

X=dataset.drop('KmeansLabel',axis=1).values
y=dataset['KmeansLabel'].values

range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters,n_init = 100,init='k-means++',random_state=0)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.Spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors)

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1],
                marker='o', c="white", alpha=1, s=200)

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()


